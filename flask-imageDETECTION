from flask import Flask
import os
import subprocess
import numpy as np
import argparse
import time
import cv2
import os

UPLOAD_FOLDER = '/home/abc/abhi/flask'

app = Flask(__name__)
app.secret_key = "secret key"
app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER
app.config['MAX_CONTENT_LENGTH'] = 16 * 1024 * 1024
import os,sys
import subprocess
#import magic
import urllib.request
# from app import app
from flask import Flask, flash, request, redirect, render_template
from werkzeug.utils import secure_filename

ALLOWED_EXTENSIONS = set(['png', 'jpg', 'jpeg', 'gif'])
# ALLOWED_EXTENSIONS = set(['txt', 'pdf', 'png', 'jpg', 'jpeg', 'gif'])
###########################################################################
###########################################################################





###########################################################################
###########################################################################
def allowed_file(filename):
    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS

@app.route('/')
def upload_form():
    return render_template('liveReport.html')

@app.route('/', methods=['POST'])
def upload_file():
    if request.method == 'POST':
        # check if the post request has the file part
        if 'file' not in request.files:
            flash('No file part << python server')
            return redirect(request.url)
        file = request.files['file']
        if file.filename == '':
            print(file.filename)
            print("file.filename")
            flash('No file selected for uploading << python server')
            return redirect(request.url)

        if file and allowed_file(file.filename):
            filename = secure_filename(file.filename)
            file.save(os.path.join(app.config['UPLOAD_FOLDER'], filename))
            flash('File successfully uploaded  /// img processing /// << python server ')
            # os.system("x.py")
            print("[]",filename)

            from PIL import Image
            import glob
            have_img=0
            while(have_img!=1):
                a=glob.glob("*.jpg")
                if(a==''):
                    have_img=0
                    print(">>  we have NO img")
                else:
                    print(a[0],">> we have img")
                    have_img=1
                    img_name=a[0]


            #python yolo.py --image images/living_room.jpg --yolo yolo-coco
            #python3 speech.py --image q.jpg --yolo yoloNN/




            # construct the argument parse and parse the arguments
            ap = argparse.ArgumentParser()
            # ap.add_argument("-i", "--image", required=True,help="path to input image")
            # ap.add_argument("-y", "--yolo", required=True,help="base path to YOLO directory")
            ap.add_argument("-c", "--confidence", type=float, default=0.5,help="minimum probability to filter weak detections")
            ap.add_argument("-t", "--threshold", type=float, default=0.3,help="threshold when applying non-maxima suppression")
            args = vars(ap.parse_args())
            # load the COCO class labels our YOLO model was trained on
            labelsPath = "yoloNN/coco.names"
            # labelsPath = os.path.sep.join([args["yolo"], "coco.names"])
            LABELS = open(labelsPath).read().strip().split("\n")

            # initialize a list of colors to represent each possible class label
            np.random.seed(42)
            COLORS = np.random.randint(0, 255, size=(len(LABELS), 3),dtype="uint8")
            # load the COCO class labels our YOLO model was trained on
            labelsPath = "yoloNN/coco.names"
            # labelsPath = os.path.sep.join([args["yolo"], "coco.names"])

            LABELS = open(labelsPath).read().strip().split("\n")

            # initialize a list of colors to represent each possible class label
            np.random.seed(42)
            COLORS = np.random.randint(0, 255, size=(len(LABELS), 3),dtype="uint8")
            # derive the paths to the YOLO weights and model configuration
            weightsPath = "yoloNN/yolov3.weights"
            configPath = "yoloNN/yolov3.cfg"
            # weightsPath = os.path.sep.join([args["yolo"], "yolov3.weights"])
            # configPath = os.path.sep.join([args["yolo"], "yolov3.cfg"])

            # load our YOLO object detector trained on COCO dataset (80 classes)
            print("[INFO] loading YOLO from disk...")
            net = cv2.dnn.readNetFromDarknet(configPath, weightsPath)
            # load our input image and grab its spatial dimensions
            image = cv2.imread(img_name)
            # image = cv2.imread(args["image"])
            (H, W) = image.shape[:2]

            # determine only the *output* layer names that we need from YOLO
            ln = net.getLayerNames()
            ln = [ln[i[0] - 1] for i in net.getUnconnectedOutLayers()]

            # construct a blob from the input image and then perform a forward
            # pass of the YOLO object detector, giving us our bounding boxes and
            # associated probabilities
            blob = cv2.dnn.blobFromImage(image, 1 / 255.0, (416, 416),swapRB=True, crop=False)
            net.setInput(blob)
            start = time.time()
            layerOutputs = net.forward(ln)
            end = time.time()

            # show timing information on YOLO
            print("[INFO] YOLO took {:.6f} seconds".format(end - start))
            # initialize our lists of detected bounding boxes, confidences, and
            # class IDs, respectively
            boxes = []
            confidences = []
            classIDs = []
            # loop over each of the layer outputs
            for output in layerOutputs:
                # loop over each of the detections
                for detection in output:
                    # extract the class ID and confidence (i.e., probability) of
                    # the current object detection
                    scores = detection[5:]
                    classID = np.argmax(scores)
                    confidence = scores[classID]

                    # filter out weak predictions by ensuring the detected
                    # probability is greater than the minimum probability
                    if confidence > args["confidence"]:
                        # scale the bounding box coordinates back relative to the
                        # size of the image, keeping in mind that YOLO actually
                        # returns the center (x, y)-coordinates of the bounding
                        # box followed by the boxes' width and height
                        box = detection[0:4] * np.array([W, H, W, H])
                        (centerX, centerY, width, height) = box.astype("int")

                        # use the center (x, y)-coordinates to derive the top and
                        # and left corner of the bounding box
                        x = int(centerX - (width / 2))
                        y = int(centerY - (height / 2))

                        # update our list of bounding box coordinates, confidences,
                        # and class IDs
                        boxes.append([x, y, int(width), int(height)])
                        confidences.append(float(confidence))
                        classIDs.append(classID)
            # apply non-maxima suppression to suppress weak, overlapping bounding
            # boxes
                idxs = cv2.dnn.NMSBoxes(boxes, confidences, args["confidence"],args["threshold"])
            # ensure at least one detection exists
                if len(idxs) > 0:
                # loop over the indexes we are keeping
                    for i in idxs.flatten():
                    # extract the bounding box coordinates
                        (x, y) = (boxes[i][0], boxes[i][1])
                        (w, h) = (boxes[i][2], boxes[i][3])

                        # draw a bounding box rectangle and label on the image
                        color = [int(c) for c in COLORS[classIDs[i]]]
                        cv2.rectangle(image, (x, y), (x + w, y + h), color, 2)
                        text = "{}: {:.4f}".format(LABELS[classIDs[i]], confidences[i])
                        print(text)
                        cv2.putText(image, text, (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX,0.5, color, 2)

            # show the output image
            os.system("rm *.jpg")
            cv2.imshow("Image", image)
            cv2.waitKey(0)
            exit(0)

            if cv2.waitKey(1) == 27:
                vs.release()
                cv2.destroyAllWindows()
                exit(0)
            cv2.waitKey(0)

            # subprocess.Popen("python3 x.py 1", shell=True)
            print("[]","going to img process")

            return redirect('/')
            # NO CODE WILL WORKE BELOW HEAR !!
        else:
            flash('Allowed file types are png,jpg,jpeg,gif !! << python server')
            return redirect(request.url)

if __name__ == "__main__":
    # app.run(debug = True)
    app.run(host = '0.0.0.0',port=5001)
